import tensorflow as tf

# LeNet-5 模型总共有 7 层，
# 第一层，卷积层
# 这一层的输入就是原始的图像像素，LeNet-5 模型接受的输入层大小为 32x32x1。
# 第一个卷积层过滤器的尺寸为 5x5, 深度为 6, 不使用全 0 填充，步长为 1。
# 因为没有使用全 0 填充，所以这一层的输出的尺寸为 32-5+1=28, 深度为 6。
# 这一个卷积层总共有 5x5x1x6+6=156 个参数，其中6个为偏置项参数。
# 因为下一层的节点矩阵有 28x28x6=4704 个节点，每个节点和 5x5=25 个当前层节点相连，
# 所以本层卷积层总共有 4704x(25+1)=122304 个连接。

# 第二层，池化层
# 这一层的输入为第一层的输出，是一个 28x28x6 的节点矩阵。本层釆用的过滤器大小为2x2, 长和宽的步长均为2,
# 所以本层的输出矩阵大小为14x14x6。

# 第三层，卷积层
# 本层的输入矩阵大小为 14x4x6, 使用的过滤器大小为5x5, 深度为 16。本层不使用全0填充.步长为 1。
# 本层的输出矩阵大小为 10x10x16。按照标准的卷积层，本层应该有5x5x6x16+16=2416个参数,10x10x16x(25+1)=41600个连接。

# 第四层，池化层
# 本层的输入矩阵大小为 10x10x16，釆用的过滤器大小为 2x2, 步长为 2。本层的输出矩阵大小为 5x5x16。

# 第五层，全连接层
# 本层的输入矩阵大小为5x5x16,在LeNet-5模型的论文中将这一层称为卷积层，但是因为过滤器的大小就是5x5,
# 所以和全连接层没有区别，在之后的TensorFlow程序实现中也会将这一层看成全连接层。
# 如果将5x5x16矩阵中的节点拉成一个向量，那么这一层和在第 4 章中介绍的全连接层输入就一样了
# 本层的输出节点个数为 120，总共有 5x5x16x120+120=48120个参数

# 第六层，全连接层
# 本层的输入节点个数为 120 个，输出节点个数为 84 个，总共参数为 120x84+84=10164个。

# 第七层，全连接层
# 本层的输入节点个数为84个，输出节点个数为10个，总共参数为84x10+10=850个。
# 上面介绍了LeNet-5模型每一层结构和设置，
# 下面给出一个TensorFlow的程序来实现一个类似 LeNet-5 模型的卷积神经网络来解决 MNIST 数字识别问题。
# 通过 TensorFlow 训练卷积神经网络的过程和第 5 章中介绍的训练全连接神经网络是完全一样的。
# 损失函数的计算、反向传播过程的实现都可以复用 5.5 节中给出的 mnistjram.py 程序。
# 唯一的区别在于因为卷积神经网络的输入层为一个三维矩阵，所以需要调整一下输入数据的格式:

# 调整输入数据placeholder的格式，输入为一个四维矩阵。
x = tf.placeholder(tf.float32, [
    BATCH_SIZE,  # 第一维表示一个batch中样例的个数。
    mnist_inference.IMAGE_SIZE,  # 第二维和第三维表示图片的尺寸。
    mnist_inference.IMAGE_SIZE,
    mnist_inference.NUM_CHANNELS  # 第四维表示图片的深度，对于RBG格式的图片，深度为 3。
],
    name='x-input')

...

# 类似地将输入的训练数据格式调整为一个四维矩阵,并将这个调整后的数据传入sess.run过程。
reshaped_xs = np.reshape(xs, (BATCH_SIZE,
                              mnist_inference.IMAGE_SIZE,
                              mnist_inference.IMAGE_SIZE,
                              mnist_inference.NUM_CHANNELS)
                         )
